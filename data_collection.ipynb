{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection <a id='data_collection'></a>\n",
    "\n",
    "Collecting data on each recipe from foodnetwork.com turned out to be a logistical nightmare, considering the time constraints of this project. Additionally, we were unable to use scrapy to extract data from each recipe page (which we will address and explain later).\n",
    "\n",
    "### A. gather all the recipe urls <a id='get_urls'></a>\n",
    "The first thing we did to collect our data was gather the recipe urls for all 65,000+ recipes at foodnetwork.com. In our first attempt to collect a list of recipe urls we used scrapy from the terminal, attempting to crawl through foodnetwork.com's a-z directory of recipes (http://www.foodnetwork.com/recipes/a-z.html). Food Network detected the use of our spiders and would either suspend us or redirect us to null pages. Our second attempt and method was to use scrapy in jupyter notebook and do the following:\n",
    "1. create a list of a-z index urls (e.g. http://www.foodnetwork.com/recipes/a-z.D.1.html)\n",
    "2. iterate through that list with scrapy, determine the range of pages for each a-z index, then create each url, and finally appending it to a list of urls (e.g. http://www.foodnetwork.com/recipes/a-z.R.23.html)\n",
    "3. lastly, iterate through that list with scrapy and extract every recipe url for each page in each a-z index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_az_urls(alphabet):\n",
    "    \n",
    "    # iterate through each letter in 'alphabet', create the a-z index urls\n",
    "    for a in alphabet:\n",
    "        url = \"http://www.foodnetwork.com/recipes/a-z.%s.1.html\" % a\n",
    "        \n",
    "        # extract the page_range for each a-z index url\n",
    "        response = requests.get(url)\n",
    "        HTML = response.text\n",
    "        page_range = Selector(text=HTML).xpath(\"//div[@class='pagination']/ul/li/a/text()\").extract()\n",
    "        # append the page_range for each letter in alphabet to a list=page_ranges\n",
    "        page_ranges.append(page_range)\n",
    "        # sleep\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # zip each letter in 'alphabet' with its corresponding page_range in page_ranges\n",
    "    for rng in page_ranges:\n",
    "        letter_ranges.append(int(rng[-1]))\n",
    "    letters_page_ranges = zip(alphabet, letter_ranges)\n",
    "    \n",
    "    # create a url for each a-z index, along with a page number, and append to a final url list=urls\n",
    "    for element in letters_page_ranges:\n",
    "        for i in range(element[1]):\n",
    "            url_ = \"http://www.foodnetwork.com/recipes/a-z.%s.%s.html\" % (element[0], (i+1))\n",
    "            urls.append(url_)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an a-z index for the argument given to get_urls\n",
    "alphabet = ['123','A','B','C','D','E','F','G','H','I','J','K','L',\n",
    "            'M','N','O','P','Q','R','S','T','V','W','XYZ']\n",
    "\n",
    "# get_az_urls(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recipe_urls(urls):\n",
    "    \n",
    "    # initiate a blank list to collect recipe urls\n",
    "    recipe_urls = []\n",
    "    \n",
    "    response = requests.get(urls)\n",
    "    HTML = response.text\n",
    "    \n",
    "    # extract recipe names\n",
    "    recipe_name = Selector(text=HTML).xpath(\"//li[@class='col18']/span[@class='arrow']/a/text()\").extract()\n",
    "    for recipe in recipe_name:\n",
    "        recipe_names.append(recipe)\n",
    "    \n",
    "    # extract recipe urls\n",
    "    recipe_url = Selector(text=HTML).xpath(\"//li[@class='col18']/span[@class='arrow']/a/@href\").extract()\n",
    "    for recipe_url_ in recipe_url:\n",
    "        recipe_urls.append('http://www.foodnetwork.com'+recipe_url_)\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    get_recipe_urls(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy was successful, rendering a final list with all 65,000+ recipe urls. We now had a starting point for collecting our final data set. The next goal was to write a function that would go through that list and extract all the information from each recipe.  \n",
    "\n",
    "### B. get the recipe data <a id='get_details'></a>\n",
    "In our first attempt to extract all the information from each recipe page, we once again wrote a function utilizing scrapy in jupyter notebook. The results and data extracted however, were incomplete. After examining the source code on foodnetwork.com's recipe pages, we determined that there was a significant amount of dynamic content on those pages operating with javascript, including some of the fields we were trying to extract. As a result, our function would move on to the next page before data we were trying to collect had fully rendered. Even after building delays into our function, the results were inconsistent and incomplete. \n",
    "\n",
    "To overcome this extraction barrier, we used the selenium webdriver. This program opened each web page and allowed it to fully render before extracting the information we needed. While it proved to be effective and successfully extracted all the fields we desired for our dataset, it took about 8 seconds for each page to fully render. With over 65,000 recipes to collect, at 8 seconds per recipe, and 3600 seconds per hour, that is over 144 hours--just over 6 days if running continuously. This is the \"logistical nightmare\" we opened this section with. In order to collect the data, we had to run 2-3 of our crawlers nearly continuously in batches, clearing our kernel cache for every 2-3 thousand recipes so the computer would not crash. The whole process took over a week, but the final result was a [63868 x 15] dataframe full of glorious recipe data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chromedriver = '/Users/kylemulle/Downloads/chromedriver' # change path as needed\n",
    "\n",
    "chop = webdriver.ChromeOptions()\n",
    "chop.add_extension('/Users/kylemulle/Downloads/AdBlock_v3.1.1.crx') # change path as needed\n",
    "\n",
    "browser = webdriver.Chrome(executable_path = chromedriver, chrome_options=chop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_details(url):\n",
    "                \n",
    "    recipe = []\n",
    "    recipe_details = []\n",
    "    \n",
    "    try: \n",
    "        browser.get(url)\n",
    "        HTML = browser.page_source\n",
    "        \n",
    "        # url\n",
    "        recipe.append(url)\n",
    "\n",
    "        # recipe name\n",
    "        name = Selector(text=HTML).xpath(\"//div[@class='tier-3 title']/h1/text()\").extract()\n",
    "        recipe.append(name)\n",
    "\n",
    "        # ingredients\n",
    "        ingredients = Selector(text=HTML).xpath(\"//ul/li/div[@class='box-block']/text()\").extract()\n",
    "        recipe.append(ingredients)\n",
    "\n",
    "        # yield\n",
    "        yield_ = Selector(text=HTML).xpath(\"//div[@class='difficulty']/dl[1]/dd/text()\").extract()\n",
    "        recipe.append(yield_)\n",
    "\n",
    "        # difficulty\n",
    "        difficulty = Selector(text=HTML).xpath(\"//div[@class='difficulty']/dl[2]/dd/text()\").extract()\n",
    "        recipe.append(difficulty)\n",
    "\n",
    "        # total time\n",
    "        time_total = Selector(text=HTML).xpath(\"//div[@class='cooking-times']/dl/dd[@class='total']/text()\").extract()\n",
    "        recipe.append(time_total)\n",
    "        \n",
    "        # preparation time\n",
    "        time_prep = Selector(text=HTML).xpath(\"//div[@class='cooking-times']/dl/dd[2]/text()\").extract()\n",
    "        recipe.append(time_prep)\n",
    "        \n",
    "        # cook time/inactive time \n",
    "        time_cook_3 = Selector(text=HTML).xpath(\"//div[@class='cooking-times']/dl/dd[3]\").extract()\n",
    "        time_cook_4 = Selector(text=HTML).xpath(\"//div[@class='cooking-times']/dl/dd[4]\").extract()\n",
    "        # w/ inactive time\n",
    "        if time_cook_4 != []:\n",
    "            recipe.append(time_cook_3) # inactive time\n",
    "            recipe.append(time_cook_4) # cook time\n",
    "        # NO inactive time\n",
    "        else:\n",
    "            recipe.append(time_cook_4) # null result\n",
    "            recipe.append(time_cook_3) # cook time\n",
    "        \n",
    "        # categories\n",
    "        categories = Selector(text=HTML).xpath(\"//ul[@class='categories']/li/a/text()\").extract()\n",
    "        recipe.append(categories)\n",
    "\n",
    "        # rating\n",
    "        rating = Selector(text=HTML).xpath(\"//a[contains(@class, 'community-rating-stars')]//@title\").extract()\n",
    "        recipe.append(rating)\n",
    "\n",
    "        # ratings\n",
    "        ratings = Selector(text=HTML).xpath(\"//div[@class='col18']/section[@class='review-rating section']/a[@class='community-rating-stars']/div[@class='gig-rating-ratingsum']/text()\").extract()\n",
    "        recipe.append(ratings)\n",
    "\n",
    "        # directions\n",
    "        directions = Selector(text=HTML).xpath(\"//ul[@class='recipe-directions-list']/li/p\").extract()\n",
    "        recipe.append(directions)\n",
    "\n",
    "        # chef\n",
    "        chef = Selector(text=HTML).xpath(\"//div[@class='col10 directions']/p[@class='copyright']/text()\").extract()\n",
    "        recipe.append(chef)\n",
    "        \n",
    "        # photo\n",
    "        image = Selector(text=HTML).xpath(\"//section[@class='single-photo-recipe']/a[@class='ico-wrap']/img/@src\").extract()\n",
    "        image_ = Selector(text=HTML).xpath(\"//div[@class='ico-wrap']/img/@src\").extract() \n",
    "        if image != []:\n",
    "            recipe.append(image)\n",
    "        else:\n",
    "            recipe.append(image_)\n",
    "\n",
    "        recipe_details.append(recipe)\n",
    "        \n",
    "    except:\n",
    "        print 'FAIL:  %s -' % (datetime.strftime(datetime.now(), '%H:%M:%S')), url\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a printable variable for keeping track of progress    \n",
    "indx = 0\n",
    "\n",
    "# first 1000 recipes in urls\n",
    "for recipe in urls[0:1000]:\n",
    "    indx += 1\n",
    "    print '%s: %s -' % (indx, datetime.strftime(datetime.now(), '%H:%M:%S')), recipe\n",
    "    get_details(recipe)\n",
    "    # sleep for a random fraction of a second\n",
    "    time.sleep(np.random.rand())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipe_dict = {}\n",
    "\n",
    "recipe_dict['url']            = [row[0] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['name']           = [row[1] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['ingredients']    = [row[2] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['yield']          = [row[3] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['difficulty']     = [row[4] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['time_total']     = [row[5] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['time_prep']      = [row[6] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['time_inactive']  = [row[7] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['time_cook']      = [row[8] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['categories']     = [row[9] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['rating']         = [row[10] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['ratings']        = [row[11] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['directions']     = [row[12] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['chef']           = [row[13] for row_no, row in enumerate(recipe_details)]\n",
    "recipe_dict['photo']          = [row[14] for row_no, row in enumerate(recipe_details)]\n",
    "\n",
    "recipe_df = pd.DataFrame(recipe_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing left to do at this point was convert appropriate index for each row in the recipe_details list to dictionary key:value pairs, and convert the dictionary into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipe_df = pd.DataFrame(recipe_dict)\n",
    "\n",
    "recipe_df.to_csv('../data_raw/recipes_0-1000.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
